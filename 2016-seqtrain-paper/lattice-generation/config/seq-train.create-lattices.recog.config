# Include corpus, lexicon, HMM like in train setup. Also LM.
include scales.config
include lexicon.config
include corpus.config
include hmm.config
include network.config

include mfcc.lda.sat.config

[*]

use-cuda			= false # this is for sprint cuda code

state-tying.type 		= cart
window-size 			= 1

[*.allophones]

add-from-lexicon            	= false
add-all                     	= true

tying-type			= global-and-nonword
nonword-phones 			= [LAUGHTER],[NOISE],[SILENCE],[VOCALIZEDNOISE]  #Gsta,Gfil,Gspe,Gint

# ---------------------------------------------------------------------------
# advanced tree search settings
[*]

search-type             	= advanced-tree-search
global-cache.file       	= $(global-cache-file)
global-cache.read-only  	= true

word-end-pruning		= 0.7	#TODO these values mostly control the size of the lattice 
beam-pruning			= 15	#TODO

beam-pruning-limit 		= 400000
word-end-pruning-limit 		= 40000

ci-cross-word-transitions       = false


# ---------------------------------------------------------------------------
# lattices
[*]

create-lattice                  = true
store-lattices                  = false #yes
lattice-pruning                 = $(lm-pruning)
lattice-pruning-limit           = infinity
optimize-lattice                = yes

# ---------------------------------------------------------------------------
# acoustic model

[*.state-tying]
type                		= decision-tree
file                            = deps/cart.tree.xml.gz
dump-state-tying.channel    	= nil


[*.mixture-set]
file                            = deps/split-8.sat.mix
scale				= $(mixture-set-scale)
normalize-mixture-weights	= false

feature-scorer-type 		= nn-trainer-feature-scorer  # Will use a trainer to get posteriors.
*.buffer-size	    		= 32
trainer				= python-trainer

prior-file			= deps/prior-f32.xml #check which prior you use
priori-scale                	= 0.7

# Python trainer config
pymod-path			= .
pymod-name			= returnn.SprintInterface
pymod-config			= epoch:12,action:forward,configfile:config-train/<baseline-config>.config # TODO paste your seed epoch and seed config file here.
# natural-pairing-layer does not matter
use-network			= true  # for std/mean normalization
target-mode			= forward-only

trainer-output-dimension 	= 1501


feature-dimension           	= 45
number-of-classes         	= 1501
